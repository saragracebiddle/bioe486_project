{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d146bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from preprocess import preprocessing\n",
    "#from read_mias import *\n",
    "import config\n",
    "from object_detector import ObjectDetector\n",
    "from custom_tensor_dataset import CustomTensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eefd00",
   "metadata": {},
   "source": [
    "Implementation References:\n",
    "\n",
    "https://pyimagesearch.com/2021/11/01/training-an-object-detector-from-scratch-in-pytorch/\n",
    "\n",
    "https://medium.com/@sharathhebbar24/object-detection-from-scratch-5ec93520adda\n",
    "\n",
    "https://www.kaggle.com/code/daniel601/pytorch-fasterrcnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0694a98",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd5d0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refno</th>\n",
       "      <th>tissue</th>\n",
       "      <th>class</th>\n",
       "      <th>severity</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>r</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mdb001</td>\n",
       "      <td>G</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>535</td>\n",
       "      <td>425</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mdb002</td>\n",
       "      <td>G</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>522</td>\n",
       "      <td>280</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdb003</td>\n",
       "      <td>D</td>\n",
       "      <td>NORM</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mdb004</td>\n",
       "      <td>D</td>\n",
       "      <td>NORM</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdb005</td>\n",
       "      <td>F</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>477</td>\n",
       "      <td>133</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    refno tissue class severity    x    y      r  image_id\n",
       "0  mdb001      G  CIRC        B  535  425  197.0         0\n",
       "1  mdb002      G  CIRC        B  522  280   69.0         1\n",
       "2  mdb003      D  NORM        N  NaN  NaN    NaN         2\n",
       "3  mdb004      D  NORM        N  NaN  NaN    NaN         3\n",
       "4  mdb005      F  CIRC        B  477  133   30.0         4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = os.getcwd()\n",
    "info = pd.read_csv(dir +'/mias_info/labels.txt', sep = ' ', header = None)\n",
    "info.columns = ['refno', 'tissue', 'class', 'severity','x','y','r']\n",
    "info['severity'] = info['severity'].fillna('N')\n",
    "info['image_id'] = info.refno.str.split('b', expand = True)[1].astype(int) -1\n",
    "info.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740efedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "\n",
    "dataset = CustomTensorDataset(data_transform)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                          batch_size=4, \n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd45b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mdb001.pgm', 'mdb002.pgm', 'mdb003.pgm', 'mdb004.pgm', 'mdb005.pgm', 'mdb006.pgm', 'mdb007.pgm', 'mdb008.pgm', 'mdb009.pgm', 'mdb010.pgm', 'mdb011.pgm', 'mdb012.pgm', 'mdb013.pgm', 'mdb014.pgm', 'mdb015.pgm', 'mdb016.pgm', 'mdb017.pgm', 'mdb018.pgm', 'mdb019.pgm', 'mdb020.pgm', 'mdb021.pgm', 'mdb022.pgm', 'mdb023.pgm', 'mdb024.pgm', 'mdb025.pgm', 'mdb026.pgm', 'mdb027.pgm', 'mdb028.pgm', 'mdb029.pgm', 'mdb030.pgm', 'mdb031.pgm', 'mdb032.pgm', 'mdb033.pgm', 'mdb034.pgm', 'mdb035.pgm', 'mdb036.pgm', 'mdb037.pgm', 'mdb038.pgm', 'mdb039.pgm', 'mdb040.pgm', 'mdb041.pgm', 'mdb042.pgm', 'mdb043.pgm', 'mdb044.pgm', 'mdb045.pgm', 'mdb046.pgm', 'mdb047.pgm', 'mdb048.pgm', 'mdb049.pgm', 'mdb050.pgm', 'mdb051.pgm', 'mdb052.pgm', 'mdb053.pgm', 'mdb054.pgm', 'mdb055.pgm', 'mdb056.pgm', 'mdb057.pgm', 'mdb058.pgm', 'mdb059.pgm', 'mdb060.pgm', 'mdb061.pgm', 'mdb062.pgm', 'mdb063.pgm', 'mdb064.pgm', 'mdb065.pgm', 'mdb066.pgm', 'mdb067.pgm', 'mdb068.pgm', 'mdb069.pgm', 'mdb070.pgm', 'mdb071.pgm', 'mdb072.pgm', 'mdb073.pgm', 'mdb074.pgm', 'mdb075.pgm', 'mdb076.pgm', 'mdb077.pgm', 'mdb078.pgm', 'mdb079.pgm', 'mdb080.pgm', 'mdb081.pgm', 'mdb082.pgm', 'mdb083.pgm', 'mdb084.pgm', 'mdb085.pgm', 'mdb086.pgm', 'mdb087.pgm', 'mdb088.pgm', 'mdb089.pgm', 'mdb090.pgm', 'mdb091.pgm', 'mdb092.pgm', 'mdb093.pgm', 'mdb094.pgm', 'mdb095.pgm', 'mdb096.pgm', 'mdb097.pgm', 'mdb098.pgm', 'mdb099.pgm', 'mdb100.pgm', 'mdb101.pgm', 'mdb102.pgm', 'mdb103.pgm', 'mdb104.pgm', 'mdb105.pgm', 'mdb106.pgm', 'mdb107.pgm', 'mdb108.pgm', 'mdb109.pgm', 'mdb110.pgm', 'mdb111.pgm', 'mdb112.pgm', 'mdb113.pgm', 'mdb114.pgm', 'mdb115.pgm', 'mdb116.pgm', 'mdb117.pgm', 'mdb118.pgm', 'mdb119.pgm', 'mdb120.pgm', 'mdb121.pgm', 'mdb122.pgm', 'mdb123.pgm', 'mdb124.pgm', 'mdb125.pgm', 'mdb126.pgm', 'mdb127.pgm', 'mdb128.pgm', 'mdb129.pgm', 'mdb130.pgm', 'mdb131.pgm', 'mdb132.pgm', 'mdb133.pgm', 'mdb134.pgm', 'mdb135.pgm', 'mdb136.pgm', 'mdb137.pgm', 'mdb138.pgm', 'mdb139.pgm', 'mdb140.pgm', 'mdb141.pgm', 'mdb142.pgm', 'mdb143.pgm', 'mdb144.pgm', 'mdb145.pgm', 'mdb146.pgm', 'mdb147.pgm', 'mdb148.pgm', 'mdb149.pgm', 'mdb150.pgm', 'mdb151.pgm', 'mdb152.pgm', 'mdb153.pgm', 'mdb154.pgm', 'mdb155.pgm', 'mdb156.pgm', 'mdb157.pgm', 'mdb158.pgm', 'mdb159.pgm', 'mdb160.pgm', 'mdb161.pgm', 'mdb162.pgm', 'mdb163.pgm', 'mdb164.pgm', 'mdb165.pgm', 'mdb166.pgm', 'mdb167.pgm', 'mdb168.pgm', 'mdb169.pgm', 'mdb170.pgm', 'mdb171.pgm', 'mdb172.pgm', 'mdb173.pgm', 'mdb174.pgm', 'mdb175.pgm', 'mdb176.pgm', 'mdb177.pgm', 'mdb178.pgm', 'mdb179.pgm', 'mdb180.pgm', 'mdb181.pgm', 'mdb182.pgm', 'mdb183.pgm', 'mdb184.pgm', 'mdb185.pgm', 'mdb186.pgm', 'mdb187.pgm', 'mdb188.pgm', 'mdb189.pgm', 'mdb190.pgm', 'mdb191.pgm', 'mdb192.pgm', 'mdb193.pgm', 'mdb194.pgm', 'mdb195.pgm', 'mdb196.pgm', 'mdb197.pgm', 'mdb198.pgm', 'mdb199.pgm', 'mdb200.pgm', 'mdb201.pgm', 'mdb202.pgm', 'mdb203.pgm', 'mdb204.pgm', 'mdb205.pgm', 'mdb206.pgm', 'mdb207.pgm', 'mdb208.pgm', 'mdb209.pgm', 'mdb210.pgm', 'mdb211.pgm', 'mdb212.pgm', 'mdb213.pgm', 'mdb214.pgm', 'mdb215.pgm', 'mdb216.pgm', 'mdb217.pgm', 'mdb218.pgm', 'mdb219.pgm', 'mdb220.pgm', 'mdb221.pgm', 'mdb222.pgm', 'mdb223.pgm', 'mdb224.pgm', 'mdb225.pgm', 'mdb226.pgm', 'mdb227.pgm', 'mdb228.pgm', 'mdb229.pgm', 'mdb230.pgm', 'mdb231.pgm', 'mdb232.pgm', 'mdb233.pgm', 'mdb234.pgm', 'mdb235.pgm', 'mdb236.pgm', 'mdb237.pgm', 'mdb238.pgm', 'mdb239.pgm', 'mdb240.pgm', 'mdb241.pgm', 'mdb242.pgm', 'mdb243.pgm', 'mdb244.pgm', 'mdb245.pgm', 'mdb246.pgm', 'mdb247.pgm', 'mdb248.pgm', 'mdb249.pgm', 'mdb250.pgm', 'mdb251.pgm', 'mdb252.pgm', 'mdb253.pgm', 'mdb254.pgm', 'mdb255.pgm', 'mdb256.pgm', 'mdb257.pgm', 'mdb258.pgm', 'mdb259.pgm', 'mdb260.pgm', 'mdb261.pgm', 'mdb262.pgm', 'mdb263.pgm', 'mdb264.pgm', 'mdb265.pgm', 'mdb266.pgm', 'mdb267.pgm', 'mdb268.pgm', 'mdb269.pgm', 'mdb270.pgm', 'mdb271.pgm', 'mdb272.pgm', 'mdb273.pgm', 'mdb274.pgm', 'mdb275.pgm', 'mdb276.pgm', 'mdb277.pgm', 'mdb278.pgm', 'mdb279.pgm', 'mdb280.pgm', 'mdb281.pgm', 'mdb282.pgm', 'mdb283.pgm', 'mdb284.pgm', 'mdb285.pgm', 'mdb286.pgm', 'mdb287.pgm', 'mdb288.pgm', 'mdb289.pgm', 'mdb290.pgm', 'mdb291.pgm', 'mdb292.pgm', 'mdb293.pgm', 'mdb294.pgm', 'mdb295.pgm', 'mdb296.pgm', 'mdb297.pgm', 'mdb298.pgm', 'mdb299.pgm', 'mdb300.pgm', 'mdb301.pgm', 'mdb302.pgm', 'mdb303.pgm', 'mdb304.pgm', 'mdb305.pgm', 'mdb306.pgm', 'mdb307.pgm', 'mdb308.pgm', 'mdb309.pgm', 'mdb310.pgm', 'mdb311.pgm', 'mdb312.pgm', 'mdb313.pgm', 'mdb314.pgm', 'mdb315.pgm', 'mdb316.pgm', 'mdb317.pgm', 'mdb318.pgm', 'mdb319.pgm', 'mdb320.pgm', 'mdb321.pgm', 'mdb322.pgm']\n",
      "322\n"
     ]
    }
   ],
   "source": [
    "from custom_tensor_dataset import list_files_walk\n",
    "files = list_files_walk('mias_data/')\n",
    "\n",
    "print(files)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d08ee2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# A set to keep track of elements that have been seen\n",
    "seen = set()\n",
    "# A list to store duplicates found in the input list\n",
    "duplicates = []\n",
    "\n",
    "# Iterate over each element in the list\n",
    "for i in files:\n",
    "    if i in seen:\n",
    "        duplicates.append(i)\n",
    "    else:\n",
    "        seen.add(i)\n",
    "\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5824c739",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mias_data\\\\mdb001.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, annotations \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m      3\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs)\n\u001b[0;32m      4\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m annotations]\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486_project\\custom_tensor_dataset.py:38\u001b[0m, in \u001b[0;36mCustomTensorDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     25\u001b[0m \t\u001b[38;5;66;03m# grab the image, label, and its bounding box coordinates\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \t\u001b[38;5;66;03m#image = self.tensors[0][index]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \t\u001b[38;5;66;03m# return a tuple of the images, labels, and bounding\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \t\u001b[38;5;66;03m# box coordinates\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \timage, target \u001b[38;5;241m=\u001b[39m generate_target(index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmias_info/labels.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m \t\timage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(image)\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486_project\\plotting.py:55\u001b[0m, in \u001b[0;36mgenerate_target\u001b[1;34m(image_id, file)\u001b[0m\n\u001b[0;32m     52\u001b[0m rows \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m image_id]\n\u001b[0;32m     54\u001b[0m imgPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmias_data\u001b[39m\u001b[38;5;124m'\u001b[39m, rows[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefno\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(imgPath)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Bounding boxes for objects\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# In coco format, bbox = [xmin, ymin, width, height]\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# In pytorch, the input should be [xmin, ymin, xmax, ymax]\u001b[39;00m\n\u001b[0;32m     60\u001b[0m boxes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\PIL\\Image.py:3465\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3462\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3465\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3466\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mias_data\\\\mdb001.png'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "for imgs, annotations in data_loader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    print(annotations)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80030177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e1efc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to C:\\Users\\Student/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:07<00:00, 21.4MB/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = get_model_instance_segmentation(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd7c23b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m    \n\u001b[0;32m     14\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, annotations \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     16\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs)\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486_project\\custom_tensor_dataset.py:38\u001b[0m, in \u001b[0;36mCustomTensorDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     25\u001b[0m \t\u001b[38;5;66;03m# grab the image, label, and its bounding box coordinates\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \t\u001b[38;5;66;03m#image = self.tensors[0][index]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \t\u001b[38;5;66;03m# return a tuple of the images, labels, and bounding\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \t\u001b[38;5;66;03m# box coordinates\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \timage, target \u001b[38;5;241m=\u001b[39m generate_target(index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmias_info/labels.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m \t\timage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(image)\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486_project\\plotting.py:54\u001b[0m, in \u001b[0;36mgenerate_target\u001b[1;34m(image_id, file)\u001b[0m\n\u001b[0;32m     50\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mrefno\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, expand \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     52\u001b[0m rows \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m image_id]\n\u001b[1;32m---> 54\u001b[0m imgPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmias\u001b[39m\u001b[38;5;124m'\u001b[39m,rows[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseverity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m], rows[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefno\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(imgPath)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Bounding boxes for objects\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# In coco format, bbox = [xmin, ymin, width, height]\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# In pytorch, the input should be [xmin, ymin, xmax, ymax]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\pandas\\core\\indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\pandas\\core\\indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "model.to(device)\n",
    "    \n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "len_dataloader = len(data_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for imgs, annotations in data_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model([imgs[0]], [annotations[0]])\n",
    "        losses = sum(loss for loss in loss_dict.values())        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "#         print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "        epoch_loss += losses\n",
    "    print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ba4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1e073df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [323, 332, 332, 323]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# partition the data into training and testing splits using 80% of\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# the data for training and the remaining 20% for testing\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m split \u001b[38;5;241m=\u001b[39m train_test_split(data, labels, bboxes, imgPaths, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.20\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# unpack the data split\u001b[39;00m\n\u001b[0;32m      5\u001b[0m (trainImages, testImages) \u001b[38;5;241m=\u001b[39m split[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2848\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2848\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2850\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2851\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2852\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2853\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:532\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \n\u001b[0;32m    504\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    531\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 532\u001b[0m check_consistent_length(\u001b[38;5;241m*\u001b[39mresult)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Student\\bioe486\\envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [323, 332, 332, 323]"
     ]
    }
   ],
   "source": [
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "split = train_test_split(data, labels, bboxes, imgPaths, test_size=0.20, random_state=42)\n",
    "# unpack the data split\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainLabels, testLabels) = split[2:4]\n",
    "(trainBBoxes, testBBoxes) = split[4:6]\n",
    "(trainPaths, testPaths) = split[6:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f8fefd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 1 at dim 1 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m (trainImages, testImages) \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(trainImages),\\\n\u001b[0;32m      3\u001b[0m \ttorch\u001b[38;5;241m.\u001b[39mtensor(testImages)\n\u001b[0;32m      4\u001b[0m (trainLabels, testLabels) \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(trainLabels),\\\n\u001b[0;32m      5\u001b[0m \ttorch\u001b[38;5;241m.\u001b[39mtensor(testLabels)\n\u001b[1;32m----> 6\u001b[0m (trainBBoxes, testBBoxes) \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(trainBBoxes),\\\n\u001b[0;32m      7\u001b[0m \ttorch\u001b[38;5;241m.\u001b[39mtensor(testBBoxes)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# define normalization transforms\u001b[39;00m\n\u001b[0;32m      9\u001b[0m transforms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     10\u001b[0m \ttransforms\u001b[38;5;241m.\u001b[39mToPILImage(),\n\u001b[0;32m     11\u001b[0m \ttransforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     12\u001b[0m \ttransforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mMEAN, std\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mSTD)\n\u001b[0;32m     13\u001b[0m ])\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 1 at dim 1 (got 2)"
     ]
    }
   ],
   "source": [
    "# convert NumPy arrays to PyTorch tensors\n",
    "(trainImages, testImages) = torch.tensor(trainImages),\\\n",
    "\ttorch.tensor(testImages)\n",
    "(trainLabels, testLabels) = torch.tensor(trainLabels),\\\n",
    "\ttorch.tensor(testLabels)\n",
    "(trainBBoxes, testBBoxes) = torch.tensor(trainBBoxes),\\\n",
    "\ttorch.tensor(testBBoxes)\n",
    "# define normalization transforms\n",
    "transforms = transforms.Compose([\n",
    "\ttransforms.ToPILImage(),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=config.MEAN, std=config.STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert NumPy arrays to PyTorch datasets\n",
    "trainDS = CustomTensorDataset((trainImages, trainLabels, trainBBoxes),\n",
    "\ttransforms=transforms)\n",
    "testDS = CustomTensorDataset((testImages, testLabels, testBBoxes),\n",
    "\ttransforms=transforms)\n",
    "print(\"[INFO] total training samples: {}...\".format(len(trainDS)))\n",
    "print(\"[INFO] total test samples: {}...\".format(len(testDS)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the testing image paths to disk so that we can use then\n",
    "# when evaluating/testing our object detector\n",
    "print(\"[INFO] saving testing image paths...\")\n",
    "f = open(config.TEST_PATHS, \"w\")\n",
    "f.write(\"\\n\".join(testPaths))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb43c76",
   "metadata": {},
   "source": [
    "Using Resnet50 pretrained weights : change as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc3294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ResNet50 network\n",
    "resnet = resnet50(pretrained=True)\n",
    "# freeze all ResNet50 layers so they will *not* be updated during the\n",
    "# training process\n",
    "for param in resnet.parameters():\n",
    "\tparam.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81ac03",
   "metadata": {},
   "source": [
    "Create object detector model and defines loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our custom object detector model and flash it to the current\n",
    "# device\n",
    "objectDetector = ObjectDetector(resnet, len(le.classes_))\n",
    "objectDetector = objectDetector.to(config.DEVICE)\n",
    "# define our loss functions\n",
    "classLossFunc = CrossEntropyLoss()\n",
    "bboxLossFunc = MSELoss()\n",
    "# initialize the optimizer, compile the model, and show the model\n",
    "# summary\n",
    "opt = Adam(objectDetector.parameters(), lr=config.INIT_LR)\n",
    "print(objectDetector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d07fc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ca282",
   "metadata": {},
   "source": [
    "`config` file currently set for\n",
    "\n",
    "20 epochs\n",
    "\n",
    "batch size 32\n",
    "\n",
    "change as necessary in the `config` before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_det import train\n",
    "H = train(objectDetector, bboxLossFunc, classLossFunc, trainDS, testDS, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model to disk\n",
    "print(\"[INFO] saving object detector model...\")\n",
    "torch.save(objectDetector, config.MODEL_PATH)\n",
    "# serialize the label encoder to disk\n",
    "print(\"[INFO] saving label encoder...\")\n",
    "f = open(config.LE_PATH, \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"total_train_loss\"], label=\"total_train_loss\")\n",
    "plt.plot(H[\"total_val_loss\"], label=\"total_val_loss\")\n",
    "plt.plot(H[\"train_class_acc\"], label=\"train_class_acc\")\n",
    "plt.plot(H[\"val_class_acc\"], label=\"val_class_acc\")\n",
    "plt.title(\"Total Training Loss and Classification Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "# save the training plot\n",
    "plotPath = os.path.sep.join([config.PLOTS_PATH, \"training.png\"])\n",
    "plt.savefig(plotPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb909d4",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e0ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python predict.py --input dataset/images/face/image_0131.jpg\n",
    "# import the necessary packages\n",
    "from torchvision import transforms\n",
    "import mimetypes\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import torch\n",
    "import cv2\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--input\", required=True,\n",
    "\thelp=\"path to input image/text file of image paths\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2bce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the input file type, but assume that we're working with\n",
    "# single input image\n",
    "filetype = mimetypes.guess_type(args[\"input\"])[0]\n",
    "imagePaths = [args[\"input\"]]\n",
    "# if the file type is a text file, then we need to process *multiple*\n",
    "# images\n",
    "if \"text/plain\" == filetype:\n",
    "\t# load the image paths in our testing file\n",
    "\timagePaths = open(args[\"input\"]).read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e707fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our object detector, set it evaluation mode, and label\n",
    "# encoder from disk\n",
    "print(\"[INFO] loading object detector...\")\n",
    "model = torch.load(config.MODEL_PATH).to(config.DEVICE)\n",
    "model.eval()\n",
    "le = pickle.loads(open(config.LE_PATH, \"rb\").read())\n",
    "# define normalization transforms\n",
    "transforms = transforms.Compose([\n",
    "\ttransforms.ToPILImage(),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=config.MEAN, std=config.STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca67193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the images that we'll be testing using our bounding box\n",
    "# regression model\n",
    "for imagePath in imagePaths:\n",
    "\t# load the image, copy it, swap its colors channels, resize it, and\n",
    "\t# bring its channel dimension forward\n",
    "\timage = cv2.imread(imagePath)\n",
    "\torig = image.copy()\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\timage = cv2.resize(image, (224, 224))\n",
    "\timage = image.transpose((2, 0, 1))\n",
    "\t# convert image to PyTorch tensor, normalize it, flash it to the\n",
    "\t# current device, and add a batch dimension\n",
    "\timage = torch.from_numpy(image)\n",
    "\timage = transforms(image).to(config.DEVICE)\n",
    "\timage = image.unsqueeze(0)\n",
    "\t\t# predict the bounding box of the object along with the class\n",
    "\t# label\n",
    "\t(boxPreds, labelPreds) = model(image)\n",
    "\t(startX, startY, endX, endY) = boxPreds[0]\n",
    "\t# determine the class label with the largest predicted\n",
    "\t# probability\n",
    "\tlabelPreds = torch.nn.Softmax(dim=-1)(labelPreds)\n",
    "\ti = labelPreds.argmax(dim=-1).cpu()\n",
    "\tlabel = le.inverse_transform(i)[0]\n",
    "\t\n",
    "\t# resize the original image such that it fits on our screen, and\n",
    "\t# grab its dimensions\n",
    "\torig = imutils.resize(orig, width=600)\n",
    "\t(h, w) = orig.shape[:2]\n",
    "\t# scale the predicted bounding box coordinates based on the image\n",
    "\t# dimensions\n",
    "\tstartX = int(startX * w)\n",
    "\tstartY = int(startY * h)\n",
    "\tendX = int(endX * w)\n",
    "\tendY = int(endY * h)\n",
    "\t# draw the predicted bounding box and class label on the image\n",
    "\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\tcv2.putText(orig, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "\t\t0.65, (0, 255, 0), 2)\n",
    "\tcv2.rectangle(orig, (startX, startY), (endX, endY),\n",
    "\t\t(0, 255, 0), 2)\n",
    "\t# show the output image \n",
    "\tcv2.imshow(\"Output\", orig)\n",
    "\tcv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
