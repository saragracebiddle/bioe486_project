{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d146bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59676b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpreprocess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mread_mias\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Student\\bioe486_project\\preprocess.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Student\\bioe486_project\\config.py:20\u001b[39m\n\u001b[32m     16\u001b[39m TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \u001b[33m\"\u001b[39m\u001b[33mtest_paths.txt\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# determine the current device and based on that set the pin memory\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# flag\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m DEVICE = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m PIN_MEMORY = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m DEVICE == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# specify ImageNet mean and standard deviation\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from preprocess import preprocessing\n",
    "from read_mias import *\n",
    "import config\n",
    "from object_detector import ObjectDetector\n",
    "from custom_tensor_dataset import CustomTensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d110fd47",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import pickle\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eefd00",
   "metadata": {},
   "source": [
    "Implementation Reference:\n",
    "\n",
    "https://pyimagesearch.com/2021/11/01/training-an-object-detector-from-scratch-in-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0694a98",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fd5d0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refno</th>\n",
       "      <th>tissue</th>\n",
       "      <th>class</th>\n",
       "      <th>severity</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mdb001</td>\n",
       "      <td>G</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>535</td>\n",
       "      <td>425</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mdb002</td>\n",
       "      <td>G</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>522</td>\n",
       "      <td>280</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdb003</td>\n",
       "      <td>D</td>\n",
       "      <td>NORM</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mdb004</td>\n",
       "      <td>D</td>\n",
       "      <td>NORM</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdb005</td>\n",
       "      <td>F</td>\n",
       "      <td>CIRC</td>\n",
       "      <td>B</td>\n",
       "      <td>477</td>\n",
       "      <td>133</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    refno tissue class severity    x    y      r\n",
       "0  mdb001      G  CIRC        B  535  425  197.0\n",
       "1  mdb002      G  CIRC        B  522  280   69.0\n",
       "2  mdb003      D  NORM        N  NaN  NaN    NaN\n",
       "3  mdb004      D  NORM        N  NaN  NaN    NaN\n",
       "4  mdb005      F  CIRC        B  477  133   30.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = os.getcwd()\n",
    "info = pd.read_csv(dir +'/mias_info/labels.txt', sep = ' ', header = None)\n",
    "info.columns = ['refno', 'tissue', 'class', 'severity','x','y','r']\n",
    "info['severity'] = info['severity'].fillna('N')\n",
    "info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1931fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, bboxes, imgPaths = preprocessing(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e073df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "split = train_test_split(data, labels, bboxes, imgPaths\n",
    "\ttest_size=0.20, random_state=42)\n",
    "# unpack the data split\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainLabels, testLabels) = split[2:4]\n",
    "(trainBBoxes, testBBoxes) = split[4:6]\n",
    "(trainPaths, testPaths) = split[6:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert NumPy arrays to PyTorch tensors\n",
    "(trainImages, testImages) = torch.tensor(trainImages),\\\n",
    "\ttorch.tensor(testImages)\n",
    "(trainLabels, testLabels) = torch.tensor(trainLabels),\\\n",
    "\ttorch.tensor(testLabels)\n",
    "(trainBBoxes, testBBoxes) = torch.tensor(trainBBoxes),\\\n",
    "\ttorch.tensor(testBBoxes)\n",
    "# define normalization transforms\n",
    "transforms = transforms.Compose([\n",
    "\ttransforms.ToPILImage(),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=config.MEAN, std=config.STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert NumPy arrays to PyTorch datasets\n",
    "trainDS = CustomTensorDataset((trainImages, trainLabels, trainBBoxes),\n",
    "\ttransforms=transforms)\n",
    "testDS = CustomTensorDataset((testImages, testLabels, testBBoxes),\n",
    "\ttransforms=transforms)\n",
    "print(\"[INFO] total training samples: {}...\".format(len(trainDS)))\n",
    "print(\"[INFO] total test samples: {}...\".format(len(testDS)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the testing image paths to disk so that we can use then\n",
    "# when evaluating/testing our object detector\n",
    "print(\"[INFO] saving testing image paths...\")\n",
    "f = open(config.TEST_PATHS, \"w\")\n",
    "f.write(\"\\n\".join(testPaths))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb43c76",
   "metadata": {},
   "source": [
    "Using Resnet50 pretrained weights : change as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc3294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ResNet50 network\n",
    "resnet = resnet50(pretrained=True)\n",
    "# freeze all ResNet50 layers so they will *not* be updated during the\n",
    "# training process\n",
    "for param in resnet.parameters():\n",
    "\tparam.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81ac03",
   "metadata": {},
   "source": [
    "Create object detector model and defines loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our custom object detector model and flash it to the current\n",
    "# device\n",
    "objectDetector = ObjectDetector(resnet, len(le.classes_))\n",
    "objectDetector = objectDetector.to(config.DEVICE)\n",
    "# define our loss functions\n",
    "classLossFunc = CrossEntropyLoss()\n",
    "bboxLossFunc = MSELoss()\n",
    "# initialize the optimizer, compile the model, and show the model\n",
    "# summary\n",
    "opt = Adam(objectDetector.parameters(), lr=config.INIT_LR)\n",
    "print(objectDetector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d07fc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ca282",
   "metadata": {},
   "source": [
    "`config` file currently set for\n",
    "\n",
    "20 epochs\n",
    "\n",
    "batch size 32\n",
    "\n",
    "change as necessary in the `config` before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_det import train\n",
    "H = train(objectDetector, bboxLossFunc, classLossFunc, trainDS, testDS, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model to disk\n",
    "print(\"[INFO] saving object detector model...\")\n",
    "torch.save(objectDetector, config.MODEL_PATH)\n",
    "# serialize the label encoder to disk\n",
    "print(\"[INFO] saving label encoder...\")\n",
    "f = open(config.LE_PATH, \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"total_train_loss\"], label=\"total_train_loss\")\n",
    "plt.plot(H[\"total_val_loss\"], label=\"total_val_loss\")\n",
    "plt.plot(H[\"train_class_acc\"], label=\"train_class_acc\")\n",
    "plt.plot(H[\"val_class_acc\"], label=\"val_class_acc\")\n",
    "plt.title(\"Total Training Loss and Classification Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "# save the training plot\n",
    "plotPath = os.path.sep.join([config.PLOTS_PATH, \"training.png\"])\n",
    "plt.savefig(plotPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb909d4",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e0ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python predict.py --input dataset/images/face/image_0131.jpg\n",
    "# import the necessary packages\n",
    "from torchvision import transforms\n",
    "import mimetypes\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import torch\n",
    "import cv2\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--input\", required=True,\n",
    "\thelp=\"path to input image/text file of image paths\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2bce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the input file type, but assume that we're working with\n",
    "# single input image\n",
    "filetype = mimetypes.guess_type(args[\"input\"])[0]\n",
    "imagePaths = [args[\"input\"]]\n",
    "# if the file type is a text file, then we need to process *multiple*\n",
    "# images\n",
    "if \"text/plain\" == filetype:\n",
    "\t# load the image paths in our testing file\n",
    "\timagePaths = open(args[\"input\"]).read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e707fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our object detector, set it evaluation mode, and label\n",
    "# encoder from disk\n",
    "print(\"[INFO] loading object detector...\")\n",
    "model = torch.load(config.MODEL_PATH).to(config.DEVICE)\n",
    "model.eval()\n",
    "le = pickle.loads(open(config.LE_PATH, \"rb\").read())\n",
    "# define normalization transforms\n",
    "transforms = transforms.Compose([\n",
    "\ttransforms.ToPILImage(),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=config.MEAN, std=config.STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca67193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the images that we'll be testing using our bounding box\n",
    "# regression model\n",
    "for imagePath in imagePaths:\n",
    "\t# load the image, copy it, swap its colors channels, resize it, and\n",
    "\t# bring its channel dimension forward\n",
    "\timage = cv2.imread(imagePath)\n",
    "\torig = image.copy()\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\timage = cv2.resize(image, (224, 224))\n",
    "\timage = image.transpose((2, 0, 1))\n",
    "\t# convert image to PyTorch tensor, normalize it, flash it to the\n",
    "\t# current device, and add a batch dimension\n",
    "\timage = torch.from_numpy(image)\n",
    "\timage = transforms(image).to(config.DEVICE)\n",
    "\timage = image.unsqueeze(0)\n",
    "\t\t# predict the bounding box of the object along with the class\n",
    "\t# label\n",
    "\t(boxPreds, labelPreds) = model(image)\n",
    "\t(startX, startY, endX, endY) = boxPreds[0]\n",
    "\t# determine the class label with the largest predicted\n",
    "\t# probability\n",
    "\tlabelPreds = torch.nn.Softmax(dim=-1)(labelPreds)\n",
    "\ti = labelPreds.argmax(dim=-1).cpu()\n",
    "\tlabel = le.inverse_transform(i)[0]\n",
    "\t\n",
    "\t# resize the original image such that it fits on our screen, and\n",
    "\t# grab its dimensions\n",
    "\torig = imutils.resize(orig, width=600)\n",
    "\t(h, w) = orig.shape[:2]\n",
    "\t# scale the predicted bounding box coordinates based on the image\n",
    "\t# dimensions\n",
    "\tstartX = int(startX * w)\n",
    "\tstartY = int(startY * h)\n",
    "\tendX = int(endX * w)\n",
    "\tendY = int(endY * h)\n",
    "\t# draw the predicted bounding box and class label on the image\n",
    "\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\tcv2.putText(orig, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "\t\t0.65, (0, 255, 0), 2)\n",
    "\tcv2.rectangle(orig, (startX, startY), (endX, endY),\n",
    "\t\t(0, 255, 0), 2)\n",
    "\t# show the output image \n",
    "\tcv2.imshow(\"Output\", orig)\n",
    "\tcv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c38110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
